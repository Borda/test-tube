{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Test Tube: Easily log and tune Deep Learning experiments Test Tube allows you to easily log metadata and track your machine learning experiments. Use Test Tube if you need to: Track many Experiments across models. Visualize and compare different experiments without uploading anywhere. Optimize your hyperparameters using grid search or random search. Automatically track ALL parameters for a particular training run. Test Tube is compatible with: Python 2 and 3 Getting started Create an Experiment from test_tube import Experiment exp = Experiment(name='dense_model', debug=False, save_dir='/Desktop/test_tube') exp.tag({'learning_rate': 0.002, 'nb_layers': 2}) for step in training_steps: tng_err = model.eval(tng_x, tng_y) exp.log('tng_err': tng_err) # training complete! # all your logs and data are ready to be visualized at testtube.williamfalcon.com Optimize your hyperparameters from test_tube import HyperOptArgumentParser # subclass of argparse parser = HyperOptArgumentParser(strategy='random_search') parser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate') # let's enable optimizing over the number of layers in the network parser.opt_list('--nb_layers', default=2, type=int, tunable=True, options=[2, 4, 8]) # and tune the number of units in each layer parser.opt_range('--neurons', default=50, type=int, tunable=True, low=100, high=800, nb_samples=10) # compile (because it's argparse underneath) hparams = parser.parse_args() # run 20 trials of random search over the hyperparams for hparam_trial in hparams.trials(20): train_network(hparam_trial) Visualize import pandas as pd import matplotlib # each experiment is saved to a metrics.csv file which can be imported anywhere # images save to exp/version/images df = pd.read_csv('../some/dir/test_tube_data/dense_model/version_0/metrics.csv') df.tng_err.plot()","title":"Test Tube: Easily log and tune Deep Learning experiments"},{"location":"#test-tube-easily-log-and-tune-deep-learning-experiments","text":"Test Tube allows you to easily log metadata and track your machine learning experiments. Use Test Tube if you need to: Track many Experiments across models. Visualize and compare different experiments without uploading anywhere. Optimize your hyperparameters using grid search or random search. Automatically track ALL parameters for a particular training run. Test Tube is compatible with: Python 2 and 3","title":"Test Tube: Easily log and tune Deep Learning experiments"},{"location":"#getting-started","text":"","title":"Getting started"},{"location":"#create-an-experiment","text":"from test_tube import Experiment exp = Experiment(name='dense_model', debug=False, save_dir='/Desktop/test_tube') exp.tag({'learning_rate': 0.002, 'nb_layers': 2}) for step in training_steps: tng_err = model.eval(tng_x, tng_y) exp.log('tng_err': tng_err) # training complete! # all your logs and data are ready to be visualized at testtube.williamfalcon.com","title":"Create an Experiment"},{"location":"#optimize-your-hyperparameters","text":"from test_tube import HyperOptArgumentParser # subclass of argparse parser = HyperOptArgumentParser(strategy='random_search') parser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate') # let's enable optimizing over the number of layers in the network parser.opt_list('--nb_layers', default=2, type=int, tunable=True, options=[2, 4, 8]) # and tune the number of units in each layer parser.opt_range('--neurons', default=50, type=int, tunable=True, low=100, high=800, nb_samples=10) # compile (because it's argparse underneath) hparams = parser.parse_args() # run 20 trials of random search over the hyperparams for hparam_trial in hparams.trials(20): train_network(hparam_trial)","title":"Optimize your hyperparameters"},{"location":"#visualize","text":"import pandas as pd import matplotlib # each experiment is saved to a metrics.csv file which can be imported anywhere # images save to exp/version/images df = pd.read_csv('../some/dir/test_tube_data/dense_model/version_0/metrics.csv') df.tng_err.plot()","title":"Visualize"},{"location":"experiment_tracking/experiment/","text":"Experiment class API An Experiment holds metadata and the results of the training run, you can instantiate an Experiment via: from test_tube import Experiment exp = Experiment(name='dense_model', debug=False, save_dir='/Desktop/test_tube') exp.tag({'learning_rate': 0.002, 'nb_layers': 2}) for step in training_steps: tng_err = model.eval(tng_x, tng_y) exp.log('tng_err': tng_err) # training complete! # all your logs and data are ready to be visualized at testtube.williamfalcon.com init options version The same Experiment can have multiple versions. Test tube generates these automatically each time you run your model. To set your own version use: exp = Experiment(name='dense_model',version=1) debug If you're debugging and don't want to create a log file turn debug to True exp = Experiment(name='dense_model',debug=True) autosave If you only want to save at the end of training, turn autosave off: exp = Experiment(name='dense_model', autosave=False) # run long training... # first time any logs are saved exp.save() create_git_tag Ever wanted a flashback to your code when you ran an experiment? Snapshot your code for this experiment using git tags: exp = Experiment(name='dense_model', create_git_tag=True) Methods tag exp.tag({k: v}) Adds an arbitrary dictionary of tags to the experiment Example exp.tag({'dataset_name': 'imagenet_1', 'learning_rate': 0.0002}) log exp.log({k:v}) Adds a row of data to the experiments Example exp.log({'val_loss': 0.22, 'epoch_nb': 1, 'batch_nb': 12}) # you can also add other rows that have separate information exp.log({'tng_loss': 0.01}) # or even a numpy array image image = np.imread('image.png') exp.log({'fake_png': image}) Saving images Example # name must have either jpg, png or jpeg in it img = np.imread('a.jpg') exp.log('test_jpg': img, 'val_err': 0.2) # saves image to ../exp/version/media/test_0.jpg # csv has file path to that image in that cell To save an image, add jpg , png or jpeg to the key corresponding with the image array. The image must be formatted the same as skimage's imsave function argparse exp.argparse(hparams) Transfers hyperparam information from Argparser or HyperOptArgumentParser Example from test_tube import HyperOptArgumentParser # parse args parser = HyperOptArgumentParser() parser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate') hparams = parser.parse_args() # learning_rate is now a meta tag for your experiment exp.argparse(hparams) save exp.save() Saves the exp to disk (including images) Example exp = Experiment(name='dense_model', autosave=False) # run long training... # first time any logs are saved exp.save()","title":"Experiment class API"},{"location":"experiment_tracking/experiment/#experiment-class-api","text":"An Experiment holds metadata and the results of the training run, you can instantiate an Experiment via: from test_tube import Experiment exp = Experiment(name='dense_model', debug=False, save_dir='/Desktop/test_tube') exp.tag({'learning_rate': 0.002, 'nb_layers': 2}) for step in training_steps: tng_err = model.eval(tng_x, tng_y) exp.log('tng_err': tng_err) # training complete! # all your logs and data are ready to be visualized at testtube.williamfalcon.com","title":"Experiment class API"},{"location":"experiment_tracking/experiment/#init-options","text":"","title":"init options"},{"location":"experiment_tracking/experiment/#version","text":"The same Experiment can have multiple versions. Test tube generates these automatically each time you run your model. To set your own version use: exp = Experiment(name='dense_model',version=1)","title":"version"},{"location":"experiment_tracking/experiment/#debug","text":"If you're debugging and don't want to create a log file turn debug to True exp = Experiment(name='dense_model',debug=True)","title":"debug"},{"location":"experiment_tracking/experiment/#autosave","text":"If you only want to save at the end of training, turn autosave off: exp = Experiment(name='dense_model', autosave=False) # run long training... # first time any logs are saved exp.save()","title":"autosave"},{"location":"experiment_tracking/experiment/#create_git_tag","text":"Ever wanted a flashback to your code when you ran an experiment? Snapshot your code for this experiment using git tags: exp = Experiment(name='dense_model', create_git_tag=True)","title":"create_git_tag"},{"location":"experiment_tracking/experiment/#methods","text":"","title":"Methods"},{"location":"experiment_tracking/experiment/#tag","text":"exp.tag({k: v}) Adds an arbitrary dictionary of tags to the experiment Example exp.tag({'dataset_name': 'imagenet_1', 'learning_rate': 0.0002})","title":"tag"},{"location":"experiment_tracking/experiment/#log","text":"exp.log({k:v}) Adds a row of data to the experiments Example exp.log({'val_loss': 0.22, 'epoch_nb': 1, 'batch_nb': 12}) # you can also add other rows that have separate information exp.log({'tng_loss': 0.01}) # or even a numpy array image image = np.imread('image.png') exp.log({'fake_png': image}) Saving images Example # name must have either jpg, png or jpeg in it img = np.imread('a.jpg') exp.log('test_jpg': img, 'val_err': 0.2) # saves image to ../exp/version/media/test_0.jpg # csv has file path to that image in that cell To save an image, add jpg , png or jpeg to the key corresponding with the image array. The image must be formatted the same as skimage's imsave function","title":"log"},{"location":"experiment_tracking/experiment/#argparse","text":"exp.argparse(hparams) Transfers hyperparam information from Argparser or HyperOptArgumentParser Example from test_tube import HyperOptArgumentParser # parse args parser = HyperOptArgumentParser() parser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate') hparams = parser.parse_args() # learning_rate is now a meta tag for your experiment exp.argparse(hparams)","title":"argparse"},{"location":"experiment_tracking/experiment/#save","text":"exp.save() Saves the exp to disk (including images) Example exp = Experiment(name='dense_model', autosave=False) # run long training... # first time any logs are saved exp.save()","title":"save"},{"location":"hyperparameter_optimization/HyperOptArgumentParser/","text":"HyperOptArgumentParser class API The HyperOptArgumentParser is a subclass of python's argparse , with added finctionality to change parameters on the fly as determined by a sampling strategy. You can instantiate an HyperOptArgumentParser via: from test_tube import HyperOptArgumentParser # subclass of argparse parser = HyperOptArgumentParser(strategy='random_search') parser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate') # let's enable optimizing over the number of layers in the network parser.opt_list('--nb_layers', default=2, type=int, tunable=True, options=[2, 4, 8]) # and tune the number of units in each layer parser.opt_range('--neurons', default=50, type=int, tunable=True, low=100, high=800, nb_samples=10) # compile (because it's argparse underneath) hparams = parser.parse_args() # run 20 trials of random search over the hyperparams for hparam_trial in hparams.trials(20): train_network(hparam_trial) init options strategy Use either random search or grid search for tuning: parser = HyperOptArgumentParser(strategy='grid_search') Methods All the functionality from argparse works but we've added the following functionality: opt_list parser.opt_list('--nb_layers', default=2, type=int, tunable=True, options=[2, 4, 8]) Enables searching over a list of values for this parameter. The tunable values ONLY replace the argparse values when running a hyperparameter optimization search. This is on purpose so your code doesn't have to change when you want to tune it. Example parser.opt_list('--nb_layers', default=2, type=int, tunable=True, options=[2, 4, 8]) hparams = parser.parse_args() # hparams.nb_layers = 2 for trial in hparams.trials(2): # trial.nb_layers is now a value in [2, 4, 8] # but hparams.nb_layers is still 2 opt_range parser.opt_range('--neurons', default=50, type=int, tunable=True, low=100, high=800, nb_samples=8, log_base=None) Enables searching over a range of values chosen randomly using the nb_samples given. The tunable values only replace the argparse values when running a hyperparameter optimization search. This is on purpose so your code doesn't have to change when you want to tune it. If log_base is set to a positive number, it will randomly search over a log scale, where the log base is log_base . This is better for search over several orders of magnitude efficiently. Example parser.opt_range('--neurons', default=50, type=int, tunable=True, low=100, high=800, nb_samples=8) hparams = parser.parse_args() # hparams.neurons = 50 for trial in hparams.trials(2): # trial.nb_layers is now a value in [100, 200, 300, 400, 500, 600 700, 800] # but hparams.neurons is still 50 json_config parser.json_config('--config', default='example.json') Replaces default values in the parser with those read from the json file Example example.json { learning_rate : 200 } parser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate') parser.json_config('--config', default='example.json') hparams = parser.parse_args() # hparams.learning_rate = 200 trials trial_generator = hparams.trials(2) Computes the trials needed for these experiments and serves them via a generator Example hparams = parser.parse_args() for trial_hparams in hparams.trials(2): # trial_hparams now has values sampled from the training routine optimize_parallel DEPRECATED... see optimize_parallel_gpu / _cpu hparams = parser.parse_args() hparams.optimize_parallel(function_to_optimize, nb_trials=20, nb_parallel=2) Parallelize the trials across nb_parallel processes. Arguments passed into the function_to_optimize are the trial_params and index of process it's in. Example # parallelize tuning on 2 gpus # this will place each trial in n into a given gpu def opt_function(trial_params, process_index): GPUs = ['0', '1'] os.environ[ CUDA_VISIBLE_DEVICES ] = GPUs[process_index] train_main(trial_params) hparams = parser.parse_args() hparams.optimize_parallel(opt_function, nb_trials=20, nb_parallel=2) # at the end of the optimize_parallel function, all 20 trials will be completed # in this case by running 10 sets of 2 trials in parallel optimize_parallel_gpu hparams = parser.parse_args() hparams.optimize_parallel_gpu(function_to_optimize, gpu_ids=['1', '0, 2'], nb_trials=20, nb_workers=2) Parallelize the trials across nb_workers processes. Auto assign the correct gpus. Argument passed into the function_to_optimize is the trial_params argument. Example # parallelize tuning on 2 gpus # this will place each trial in n into a given gpu def train_main(trial_params): # train your model, etc here... hparams = parser.parse_args() hparams.optimize_parallel_gpu(train_main, gpu_ids=['1', '0, 2'], nb_trials=20, nb_workers=2) # at the end of the optimize_parallel function, all 20 trials will be completed # in this case by running 10 sets of 2 trials in parallel optimize_parallel_cpu hparams = parser.parse_args() hparams.optimize_parallel_cpu(function_to_optimize, nb_trials=20, nb_workers=2) Parallelize the trials across nb_workers cpus. Argument passed into the function_to_optimize is the trial_params argument. Example # parallelize tuning on 2 cpus # this will place each trial in n into a given gpu def train_main(trial_params): # train your model, etc here... hparams = parser.parse_args() hparams.optimize_parallel_cpu(train_main, nb_trials=20, nb_workers=2) # at the end of the optimize_parallel function, all 20 trials will be completed # in this case by running 10 sets of 2 trials in parallel","title":"HyperOptArgumentParser class API"},{"location":"hyperparameter_optimization/HyperOptArgumentParser/#hyperoptargumentparser-class-api","text":"The HyperOptArgumentParser is a subclass of python's argparse , with added finctionality to change parameters on the fly as determined by a sampling strategy. You can instantiate an HyperOptArgumentParser via: from test_tube import HyperOptArgumentParser # subclass of argparse parser = HyperOptArgumentParser(strategy='random_search') parser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate') # let's enable optimizing over the number of layers in the network parser.opt_list('--nb_layers', default=2, type=int, tunable=True, options=[2, 4, 8]) # and tune the number of units in each layer parser.opt_range('--neurons', default=50, type=int, tunable=True, low=100, high=800, nb_samples=10) # compile (because it's argparse underneath) hparams = parser.parse_args() # run 20 trials of random search over the hyperparams for hparam_trial in hparams.trials(20): train_network(hparam_trial)","title":"HyperOptArgumentParser class API"},{"location":"hyperparameter_optimization/HyperOptArgumentParser/#init-options","text":"","title":"init options"},{"location":"hyperparameter_optimization/HyperOptArgumentParser/#strategy","text":"Use either random search or grid search for tuning: parser = HyperOptArgumentParser(strategy='grid_search')","title":"strategy"},{"location":"hyperparameter_optimization/HyperOptArgumentParser/#methods","text":"All the functionality from argparse works but we've added the following functionality:","title":"Methods"},{"location":"hyperparameter_optimization/HyperOptArgumentParser/#opt_list","text":"parser.opt_list('--nb_layers', default=2, type=int, tunable=True, options=[2, 4, 8]) Enables searching over a list of values for this parameter. The tunable values ONLY replace the argparse values when running a hyperparameter optimization search. This is on purpose so your code doesn't have to change when you want to tune it. Example parser.opt_list('--nb_layers', default=2, type=int, tunable=True, options=[2, 4, 8]) hparams = parser.parse_args() # hparams.nb_layers = 2 for trial in hparams.trials(2): # trial.nb_layers is now a value in [2, 4, 8] # but hparams.nb_layers is still 2","title":"opt_list"},{"location":"hyperparameter_optimization/HyperOptArgumentParser/#opt_range","text":"parser.opt_range('--neurons', default=50, type=int, tunable=True, low=100, high=800, nb_samples=8, log_base=None) Enables searching over a range of values chosen randomly using the nb_samples given. The tunable values only replace the argparse values when running a hyperparameter optimization search. This is on purpose so your code doesn't have to change when you want to tune it. If log_base is set to a positive number, it will randomly search over a log scale, where the log base is log_base . This is better for search over several orders of magnitude efficiently. Example parser.opt_range('--neurons', default=50, type=int, tunable=True, low=100, high=800, nb_samples=8) hparams = parser.parse_args() # hparams.neurons = 50 for trial in hparams.trials(2): # trial.nb_layers is now a value in [100, 200, 300, 400, 500, 600 700, 800] # but hparams.neurons is still 50","title":"opt_range"},{"location":"hyperparameter_optimization/HyperOptArgumentParser/#json_config","text":"parser.json_config('--config', default='example.json') Replaces default values in the parser with those read from the json file Example example.json { learning_rate : 200 } parser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate') parser.json_config('--config', default='example.json') hparams = parser.parse_args() # hparams.learning_rate = 200","title":"json_config"},{"location":"hyperparameter_optimization/HyperOptArgumentParser/#trials","text":"trial_generator = hparams.trials(2) Computes the trials needed for these experiments and serves them via a generator Example hparams = parser.parse_args() for trial_hparams in hparams.trials(2): # trial_hparams now has values sampled from the training routine","title":"trials"},{"location":"hyperparameter_optimization/HyperOptArgumentParser/#optimize_parallel","text":"DEPRECATED... see optimize_parallel_gpu / _cpu hparams = parser.parse_args() hparams.optimize_parallel(function_to_optimize, nb_trials=20, nb_parallel=2) Parallelize the trials across nb_parallel processes. Arguments passed into the function_to_optimize are the trial_params and index of process it's in. Example # parallelize tuning on 2 gpus # this will place each trial in n into a given gpu def opt_function(trial_params, process_index): GPUs = ['0', '1'] os.environ[ CUDA_VISIBLE_DEVICES ] = GPUs[process_index] train_main(trial_params) hparams = parser.parse_args() hparams.optimize_parallel(opt_function, nb_trials=20, nb_parallel=2) # at the end of the optimize_parallel function, all 20 trials will be completed # in this case by running 10 sets of 2 trials in parallel","title":"optimize_parallel"},{"location":"hyperparameter_optimization/HyperOptArgumentParser/#optimize_parallel_gpu","text":"hparams = parser.parse_args() hparams.optimize_parallel_gpu(function_to_optimize, gpu_ids=['1', '0, 2'], nb_trials=20, nb_workers=2) Parallelize the trials across nb_workers processes. Auto assign the correct gpus. Argument passed into the function_to_optimize is the trial_params argument. Example # parallelize tuning on 2 gpus # this will place each trial in n into a given gpu def train_main(trial_params): # train your model, etc here... hparams = parser.parse_args() hparams.optimize_parallel_gpu(train_main, gpu_ids=['1', '0, 2'], nb_trials=20, nb_workers=2) # at the end of the optimize_parallel function, all 20 trials will be completed # in this case by running 10 sets of 2 trials in parallel","title":"optimize_parallel_gpu"},{"location":"hyperparameter_optimization/HyperOptArgumentParser/#optimize_parallel_cpu","text":"hparams = parser.parse_args() hparams.optimize_parallel_cpu(function_to_optimize, nb_trials=20, nb_workers=2) Parallelize the trials across nb_workers cpus. Argument passed into the function_to_optimize is the trial_params argument. Example # parallelize tuning on 2 cpus # this will place each trial in n into a given gpu def train_main(trial_params): # train your model, etc here... hparams = parser.parse_args() hparams.optimize_parallel_cpu(train_main, nb_trials=20, nb_workers=2) # at the end of the optimize_parallel function, all 20 trials will be completed # in this case by running 10 sets of 2 trials in parallel","title":"optimize_parallel_cpu"}]}