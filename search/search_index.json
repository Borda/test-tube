{
    "docs": [
        {
            "location": "/",
            "text": "Test Tube: Easily log and tune Deep Learning experiments\n\n\nTest Tube allows you to easily log metadata and track your machine\nlearning experiments.\n\n\nUse Test Tube if you need to:\n\n\n\n\nTrack many \nExperiments\n across\n    models.\n\n\nVisualize\n and compare different\n    experiments without uploading anywhere.\n\n\nOptimize your\n    hyperparameters\n\n    using grid search or random search.\n\n\nAutomatically track ALL parameters for a particular training run.\n\n\n\n\nTest Tube is compatible with: Python 2 and 3\n\n\nGetting started\n\n\n\n\nCreate an \nExperiment\n\n\nfrom test_tube import Experiment\n\nexp = Experiment(name='dense_model',\n                 debug=False,\n                 save_dir='/Desktop/test_tube')\n\nexp.tag({'learning_rate': 0.002, 'nb_layers': 2})\n\nfor step in training_steps:\n    tng_err = model.eval(tng_x, tng_y)\n\n    exp.log('tng_err': tng_err)\n\n# training complete!\n# all your logs and data are ready to be visualized at testtube.williamfalcon.com\n\n\n\n\n\n\nOptimize your \nhyperparameters\n\n\nfrom test_tube import HyperOptArgumentParser\n\n# subclass of argparse\nparser = HyperOptArgumentParser(strategy='random_search')\nparser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate')\n\n# let's enable optimizing over the number of layers in the network\nparser.opt_list('--nb_layers', default=2, type=int, tunable=True, options=[2, 4, 8])\n\n# and tune the number of units in each layer\nparser.opt_range('--neurons', default=50, type=int, tunable=True, low=100, high=800, nb_samples=10)\n\n# compile (because it's argparse underneath)\nhparams = parser.parse_args()\n\n# run 20 trials of random search over the hyperparams\nfor hparam_trial in hparams.trials(20):\n    train_network(hparam_trial)\n\n\n\n\n\n\nVisualize\n\n\nimport pandas as pd\nimport matplotlib\n\n# each experiment is saved to a metrics.csv file which can be imported anywhere\n# images save to exp/version/images\ndf = pd.read_csv('../some/dir/test_tube_data/dense_model/version_0/metrics.csv')\ndf.tng_err.plot()",
            "title": "Test Tube: Easily log and tune Deep Learning experiments"
        },
        {
            "location": "/#test-tube-easily-log-and-tune-deep-learning-experiments",
            "text": "Test Tube allows you to easily log metadata and track your machine\nlearning experiments.  Use Test Tube if you need to:   Track many  Experiments  across\n    models.  Visualize  and compare different\n    experiments without uploading anywhere.  Optimize your\n    hyperparameters \n    using grid search or random search.  Automatically track ALL parameters for a particular training run.   Test Tube is compatible with: Python 2 and 3",
            "title": "Test Tube: Easily log and tune Deep Learning experiments"
        },
        {
            "location": "/#getting-started",
            "text": "",
            "title": "Getting started"
        },
        {
            "location": "/#create-an-experiment",
            "text": "from test_tube import Experiment\n\nexp = Experiment(name='dense_model',\n                 debug=False,\n                 save_dir='/Desktop/test_tube')\n\nexp.tag({'learning_rate': 0.002, 'nb_layers': 2})\n\nfor step in training_steps:\n    tng_err = model.eval(tng_x, tng_y)\n\n    exp.log('tng_err': tng_err)\n\n# training complete!\n# all your logs and data are ready to be visualized at testtube.williamfalcon.com",
            "title": "Create an Experiment"
        },
        {
            "location": "/#optimize-your-hyperparameters",
            "text": "from test_tube import HyperOptArgumentParser\n\n# subclass of argparse\nparser = HyperOptArgumentParser(strategy='random_search')\nparser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate')\n\n# let's enable optimizing over the number of layers in the network\nparser.opt_list('--nb_layers', default=2, type=int, tunable=True, options=[2, 4, 8])\n\n# and tune the number of units in each layer\nparser.opt_range('--neurons', default=50, type=int, tunable=True, low=100, high=800, nb_samples=10)\n\n# compile (because it's argparse underneath)\nhparams = parser.parse_args()\n\n# run 20 trials of random search over the hyperparams\nfor hparam_trial in hparams.trials(20):\n    train_network(hparam_trial)",
            "title": "Optimize your hyperparameters"
        },
        {
            "location": "/#visualize",
            "text": "import pandas as pd\nimport matplotlib\n\n# each experiment is saved to a metrics.csv file which can be imported anywhere\n# images save to exp/version/images\ndf = pd.read_csv('../some/dir/test_tube_data/dense_model/version_0/metrics.csv')\ndf.tng_err.plot()",
            "title": "Visualize"
        },
        {
            "location": "/experiment_tracking/experiment/",
            "text": "Experiment class API\n\n\nAn Experiment holds metadata and the results of the training run, you\ncan instantiate an \nExperiment\n via:\n\n\nfrom test_tube import Experiment\n\nexp = Experiment(name='dense_model',\n                 debug=False,\n                 save_dir='/Desktop/test_tube')\n\nexp.tag({'learning_rate': 0.002, 'nb_layers': 2})\n\nfor step in training_steps:\n    tng_err = model.eval(tng_x, tng_y)\n\n    exp.log('tng_err': tng_err)\n\n# training complete!\n# all your logs and data are ready to be visualized at testtube.williamfalcon.com\n\n\n\n\n\n\ninit options\n\n\nversion\n\n\nThe same Experiment can have multiple versions. Test tube generates\nthese automatically each time you run your model. To set your own\nversion use:\n\n\nexp = Experiment(name='dense_model',version=1)\n\n\n\n\ndebug\n\n\nIf you're debugging and don't want to create a log file turn debug to\nTrue\n\n\nexp = Experiment(name='dense_model',debug=True)\n\n\n\n\nautosave\n\n\nIf you only want to save at the end of training, turn autosave off:\n\n\nexp = Experiment(name='dense_model', autosave=False)\n\n# run long training...\n\n# first time any logs are saved\nexp.save()\n\n\n\n\ncreate_git_tag\n\n\nEver wanted a flashback to your code when you ran an experiment?\nSnapshot your code for this experiment using git tags:\n\n\nexp = Experiment(name='dense_model', create_git_tag=True)\n\n\n\n\n\n\nMethods\n\n\ntag\n\n\nexp.tag({k: v})\n\n\n\n\nAdds an arbitrary dictionary of tags to the experiment\n\n\nExample\n\n\nexp.tag({'dataset_name': 'imagenet_1', 'learning_rate': 0.0002})\n\n\n\n\nlog\n\n\nexp.log({k:v})\n\n\n\n\nAdds a row of data to the experiments\n\n\nExample\n\n\nexp.log({'val_loss': 0.22, 'epoch_nb': 1, 'batch_nb': 12})\n\n# you can also add other rows that have separate information\nexp.log({'tng_loss': 0.01})\n\n# or even a numpy array image\nimage = np.imread('image.png')\nexp.log({'fake_png': image})\n\n\n\n\nSaving images Example\n\n\n# name must have either jpg, png or jpeg in it\nimg = np.imread('a.jpg')\nexp.log('test_jpg': img, 'val_err': 0.2)\n\n# saves image to ../exp/version/media/test_0.jpg\n# csv has file path to that image in that cell\n\n\n\n\nTo save an image, add \njpg\n, \npng\n or \njpeg\n to the key corresponding\nwith the image array. The image must be formatted the same as skimage's\n\nimsave\n\nfunction\n\n\nargparse\n\n\nexp.argparse(hparams)\n\n\n\n\nTransfers hyperparam information from Argparser or\nHyperOptArgumentParser\n\n\nExample\n\n\nfrom test_tube import HyperOptArgumentParser\n\n# parse args\nparser = HyperOptArgumentParser()\nparser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate')\nhparams = parser.parse_args()\n\n# learning_rate is now a meta tag for your experiment\nexp.argparse(hparams)\n\n\n\n\nsave\n\n\nexp.save()\n\n\n\n\nSaves the exp to disk (including images)\n\n\nExample\n\n\nexp = Experiment(name='dense_model', autosave=False)\n\n# run long training...\n\n# first time any logs are saved\nexp.save()",
            "title": "Experiment class API"
        },
        {
            "location": "/experiment_tracking/experiment/#experiment-class-api",
            "text": "An Experiment holds metadata and the results of the training run, you\ncan instantiate an  Experiment  via:  from test_tube import Experiment\n\nexp = Experiment(name='dense_model',\n                 debug=False,\n                 save_dir='/Desktop/test_tube')\n\nexp.tag({'learning_rate': 0.002, 'nb_layers': 2})\n\nfor step in training_steps:\n    tng_err = model.eval(tng_x, tng_y)\n\n    exp.log('tng_err': tng_err)\n\n# training complete!\n# all your logs and data are ready to be visualized at testtube.williamfalcon.com",
            "title": "Experiment class API"
        },
        {
            "location": "/experiment_tracking/experiment/#init-options",
            "text": "",
            "title": "init options"
        },
        {
            "location": "/experiment_tracking/experiment/#version",
            "text": "The same Experiment can have multiple versions. Test tube generates\nthese automatically each time you run your model. To set your own\nversion use:  exp = Experiment(name='dense_model',version=1)",
            "title": "version"
        },
        {
            "location": "/experiment_tracking/experiment/#debug",
            "text": "If you're debugging and don't want to create a log file turn debug to\nTrue  exp = Experiment(name='dense_model',debug=True)",
            "title": "debug"
        },
        {
            "location": "/experiment_tracking/experiment/#autosave",
            "text": "If you only want to save at the end of training, turn autosave off:  exp = Experiment(name='dense_model', autosave=False)\n\n# run long training...\n\n# first time any logs are saved\nexp.save()",
            "title": "autosave"
        },
        {
            "location": "/experiment_tracking/experiment/#create_git_tag",
            "text": "Ever wanted a flashback to your code when you ran an experiment?\nSnapshot your code for this experiment using git tags:  exp = Experiment(name='dense_model', create_git_tag=True)",
            "title": "create_git_tag"
        },
        {
            "location": "/experiment_tracking/experiment/#methods",
            "text": "",
            "title": "Methods"
        },
        {
            "location": "/experiment_tracking/experiment/#tag",
            "text": "exp.tag({k: v})  Adds an arbitrary dictionary of tags to the experiment  Example  exp.tag({'dataset_name': 'imagenet_1', 'learning_rate': 0.0002})",
            "title": "tag"
        },
        {
            "location": "/experiment_tracking/experiment/#log",
            "text": "exp.log({k:v})  Adds a row of data to the experiments  Example  exp.log({'val_loss': 0.22, 'epoch_nb': 1, 'batch_nb': 12})\n\n# you can also add other rows that have separate information\nexp.log({'tng_loss': 0.01})\n\n# or even a numpy array image\nimage = np.imread('image.png')\nexp.log({'fake_png': image})  Saving images Example  # name must have either jpg, png or jpeg in it\nimg = np.imread('a.jpg')\nexp.log('test_jpg': img, 'val_err': 0.2)\n\n# saves image to ../exp/version/media/test_0.jpg\n# csv has file path to that image in that cell  To save an image, add  jpg ,  png  or  jpeg  to the key corresponding\nwith the image array. The image must be formatted the same as skimage's imsave \nfunction",
            "title": "log"
        },
        {
            "location": "/experiment_tracking/experiment/#argparse",
            "text": "exp.argparse(hparams)  Transfers hyperparam information from Argparser or\nHyperOptArgumentParser  Example  from test_tube import HyperOptArgumentParser\n\n# parse args\nparser = HyperOptArgumentParser()\nparser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate')\nhparams = parser.parse_args()\n\n# learning_rate is now a meta tag for your experiment\nexp.argparse(hparams)",
            "title": "argparse"
        },
        {
            "location": "/experiment_tracking/experiment/#save",
            "text": "exp.save()  Saves the exp to disk (including images)  Example  exp = Experiment(name='dense_model', autosave=False)\n\n# run long training...\n\n# first time any logs are saved\nexp.save()",
            "title": "save"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/",
            "text": "HyperOptArgumentParser class API\n\n\nThe HyperOptArgumentParser is a subclass of python's\n\nargparse\n, with added\nfinctionality to change parameters on the fly as determined by a\nsampling strategy.\n\n\nYou can instantiate an \nHyperOptArgumentParser\n via:\n\n\nfrom test_tube import HyperOptArgumentParser\n\n# subclass of argparse\nparser = HyperOptArgumentParser(strategy='random_search')\nparser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate')\n\n# let's enable optimizing over the number of layers in the network\nparser.opt_list('--nb_layers', default=2, type=int, tunable=True, options=[2, 4, 8])\n\n# and tune the number of units in each layer\nparser.opt_range('--neurons', default=50, type=int, tunable=True, low=100, high=800, nb_samples=10)\n\n# compile (because it's argparse underneath)\nhparams = parser.parse_args()\n\n# run 20 trials of random search over the hyperparams\nfor hparam_trial in hparams.trials(20):\n    train_network(hparam_trial)\n\n\n\n\n\n\ninit options\n\n\nstrategy\n\n\nUse either \nrandom\nsearch\n\nor \ngrid\nsearch\n\nfor tuning:\n\n\nparser = HyperOptArgumentParser(strategy='grid_search')\n\n\n\n\n\n\nMethods\n\n\nAll the functionality from argparse works but we've added the following\nfunctionality:\n\n\nopt_list\n\n\nparser.opt_list('--nb_layers', default=2, type=int, tunable=True, options=[2, 4, 8])\n\n\n\n\nEnables searching over a list of values for this parameter. The tunable\nvalues ONLY replace the argparse values when running a hyperparameter\noptimization search. This is on purpose so your code doesn't have to\nchange when you want to tune it.\n\n\nExample\n\n\nparser.opt_list('--nb_layers', default=2, type=int, tunable=True, options=[2, 4, 8])\nhparams = parser.parse_args()\n# hparams.nb_layers = 2\n\nfor trial in hparams.trials(2):\n    # trial.nb_layers is now a value in [2, 4, 8]\n    # but hparams.nb_layers is still 2\n\n\n\n\nopt_range\n\n\nparser.opt_range('--neurons', default=50, type=int, tunable=True, low=100, high=800, nb_samples=8, log_base=None)\n\n\n\n\nEnables searching over a range of values chosen randomly using the\n\nnb_samples\n given. The tunable values \nonly\n replace the argparse\nvalues when running a hyperparameter optimization search. This is on\npurpose so your code doesn't have to change when you want to tune it.\n\n\nIf \nlog_base\n is set to a positive number, it will randomly search over\na log scale, where the log base is \nlog_base\n. This is better for search\nover several orders of magnitude efficiently.\n\n\nExample\n\n\nparser.opt_range('--neurons', default=50, type=int, tunable=True, low=100, high=800, nb_samples=8)\nhparams = parser.parse_args()\n# hparams.neurons = 50\n\nfor trial in hparams.trials(2):\n    # trial.nb_layers is now a value in [100, 200, 300, 400, 500, 600 700, 800]\n    # but hparams.neurons is still 50\n\n\n\n\njson_config\n\n\nparser.json_config('--config', default='example.json')\n\n\n\n\nReplaces default values in the parser with those read from the json file\n\n\nExample\n\n\nexample.json\n\n\n{\n    \"learning_rate\": 200\n}\n\n\n\n\nparser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate')\nparser.json_config('--config', default='example.json')\nhparams = parser.parse_args()\n\n# hparams.learning_rate = 200\n\n\n\n\ntrials\n\n\ntrial_generator = hparams.trials(2)\n\n\n\n\nComputes the trials needed for these experiments and serves them via a\ngenerator\n\n\nExample\n\n\nhparams = parser.parse_args()\nfor trial_hparams in hparams.trials(2):\n    # trial_hparams now has values sampled from the training routine\n\n\n\n\noptimize_parallel\n\n\nDEPRECATED... see optimize_parallel_gpu / _cpu\n\n\nhparams = parser.parse_args()\nhparams.optimize_parallel(function_to_optimize, nb_trials=20, nb_parallel=2)\n\n\n\n\nParallelize the trials across \nnb_parallel\n processes. Arguments passed\ninto the \nfunction_to_optimize\n are the \ntrial_params\n and index of\nprocess it's in.\n\n\nExample\n\n\n# parallelize tuning on 2 gpus\n# this will place each trial in n into a given gpu\ndef opt_function(trial_params, process_index):\n    GPUs = ['0', '1']\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPUs[process_index]\n    train_main(trial_params)\n\nhparams = parser.parse_args()\nhparams.optimize_parallel(opt_function, nb_trials=20, nb_parallel=2)\n\n# at the end of the optimize_parallel function, all 20 trials will be completed\n# in this case by running 10 sets of 2 trials in parallel\n\n\n\n\noptimize_parallel_gpu\n\n\nhparams = parser.parse_args()\nhparams.optimize_parallel_gpu(function_to_optimize, gpu_ids=['1', '0, 2'], nb_trials=20, nb_workers=2)\n\n\n\n\nParallelize the trials across \nnb_workers\n processes. Auto assign the\ncorrect gpus. Argument passed into the \nfunction_to_optimize\n is the\n\ntrial_params\n argument.\n\n\nExample\n\n\n# parallelize tuning on 2 gpus\n# this will place each trial in n into a given gpu\ndef train_main(trial_params):\n    # train your model, etc here...\n\nhparams = parser.parse_args()\nhparams.optimize_parallel_gpu(train_main, gpu_ids=['1', '0, 2'], nb_trials=20, nb_workers=2)\n\n# at the end of the optimize_parallel function, all 20 trials will be completed\n# in this case by running 10 sets of 2 trials in parallel\n\n\n\n\noptimize_parallel_cpu\n\n\nhparams = parser.parse_args()\nhparams.optimize_parallel_cpu(function_to_optimize, nb_trials=20, nb_workers=2)\n\n\n\n\nParallelize the trials across \nnb_workers\n cpus. Argument passed into\nthe \nfunction_to_optimize\n is the \ntrial_params\n argument.\n\n\nExample\n\n\n# parallelize tuning on 2 cpus\n# this will place each trial in n into a given gpu\ndef train_main(trial_params):\n    # train your model, etc here...\n\nhparams = parser.parse_args()\nhparams.optimize_parallel_cpu(train_main, nb_trials=20, nb_workers=2)\n\n# at the end of the optimize_parallel function, all 20 trials will be completed\n# in this case by running 10 sets of 2 trials in parallel",
            "title": "HyperOptArgumentParser class API"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/#hyperoptargumentparser-class-api",
            "text": "The HyperOptArgumentParser is a subclass of python's argparse , with added\nfinctionality to change parameters on the fly as determined by a\nsampling strategy.  You can instantiate an  HyperOptArgumentParser  via:  from test_tube import HyperOptArgumentParser\n\n# subclass of argparse\nparser = HyperOptArgumentParser(strategy='random_search')\nparser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate')\n\n# let's enable optimizing over the number of layers in the network\nparser.opt_list('--nb_layers', default=2, type=int, tunable=True, options=[2, 4, 8])\n\n# and tune the number of units in each layer\nparser.opt_range('--neurons', default=50, type=int, tunable=True, low=100, high=800, nb_samples=10)\n\n# compile (because it's argparse underneath)\nhparams = parser.parse_args()\n\n# run 20 trials of random search over the hyperparams\nfor hparam_trial in hparams.trials(20):\n    train_network(hparam_trial)",
            "title": "HyperOptArgumentParser class API"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/#init-options",
            "text": "",
            "title": "init options"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/#strategy",
            "text": "Use either  random\nsearch \nor  grid\nsearch \nfor tuning:  parser = HyperOptArgumentParser(strategy='grid_search')",
            "title": "strategy"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/#methods",
            "text": "All the functionality from argparse works but we've added the following\nfunctionality:",
            "title": "Methods"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/#opt_list",
            "text": "parser.opt_list('--nb_layers', default=2, type=int, tunable=True, options=[2, 4, 8])  Enables searching over a list of values for this parameter. The tunable\nvalues ONLY replace the argparse values when running a hyperparameter\noptimization search. This is on purpose so your code doesn't have to\nchange when you want to tune it.  Example  parser.opt_list('--nb_layers', default=2, type=int, tunable=True, options=[2, 4, 8])\nhparams = parser.parse_args()\n# hparams.nb_layers = 2\n\nfor trial in hparams.trials(2):\n    # trial.nb_layers is now a value in [2, 4, 8]\n    # but hparams.nb_layers is still 2",
            "title": "opt_list"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/#opt_range",
            "text": "parser.opt_range('--neurons', default=50, type=int, tunable=True, low=100, high=800, nb_samples=8, log_base=None)  Enables searching over a range of values chosen randomly using the nb_samples  given. The tunable values  only  replace the argparse\nvalues when running a hyperparameter optimization search. This is on\npurpose so your code doesn't have to change when you want to tune it.  If  log_base  is set to a positive number, it will randomly search over\na log scale, where the log base is  log_base . This is better for search\nover several orders of magnitude efficiently.  Example  parser.opt_range('--neurons', default=50, type=int, tunable=True, low=100, high=800, nb_samples=8)\nhparams = parser.parse_args()\n# hparams.neurons = 50\n\nfor trial in hparams.trials(2):\n    # trial.nb_layers is now a value in [100, 200, 300, 400, 500, 600 700, 800]\n    # but hparams.neurons is still 50",
            "title": "opt_range"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/#json_config",
            "text": "parser.json_config('--config', default='example.json')  Replaces default values in the parser with those read from the json file  Example  example.json  {\n    \"learning_rate\": 200\n}  parser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate')\nparser.json_config('--config', default='example.json')\nhparams = parser.parse_args()\n\n# hparams.learning_rate = 200",
            "title": "json_config"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/#trials",
            "text": "trial_generator = hparams.trials(2)  Computes the trials needed for these experiments and serves them via a\ngenerator  Example  hparams = parser.parse_args()\nfor trial_hparams in hparams.trials(2):\n    # trial_hparams now has values sampled from the training routine",
            "title": "trials"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/#optimize_parallel",
            "text": "DEPRECATED... see optimize_parallel_gpu / _cpu  hparams = parser.parse_args()\nhparams.optimize_parallel(function_to_optimize, nb_trials=20, nb_parallel=2)  Parallelize the trials across  nb_parallel  processes. Arguments passed\ninto the  function_to_optimize  are the  trial_params  and index of\nprocess it's in.  Example  # parallelize tuning on 2 gpus\n# this will place each trial in n into a given gpu\ndef opt_function(trial_params, process_index):\n    GPUs = ['0', '1']\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPUs[process_index]\n    train_main(trial_params)\n\nhparams = parser.parse_args()\nhparams.optimize_parallel(opt_function, nb_trials=20, nb_parallel=2)\n\n# at the end of the optimize_parallel function, all 20 trials will be completed\n# in this case by running 10 sets of 2 trials in parallel",
            "title": "optimize_parallel"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/#optimize_parallel_gpu",
            "text": "hparams = parser.parse_args()\nhparams.optimize_parallel_gpu(function_to_optimize, gpu_ids=['1', '0, 2'], nb_trials=20, nb_workers=2)  Parallelize the trials across  nb_workers  processes. Auto assign the\ncorrect gpus. Argument passed into the  function_to_optimize  is the trial_params  argument.  Example  # parallelize tuning on 2 gpus\n# this will place each trial in n into a given gpu\ndef train_main(trial_params):\n    # train your model, etc here...\n\nhparams = parser.parse_args()\nhparams.optimize_parallel_gpu(train_main, gpu_ids=['1', '0, 2'], nb_trials=20, nb_workers=2)\n\n# at the end of the optimize_parallel function, all 20 trials will be completed\n# in this case by running 10 sets of 2 trials in parallel",
            "title": "optimize_parallel_gpu"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/#optimize_parallel_cpu",
            "text": "hparams = parser.parse_args()\nhparams.optimize_parallel_cpu(function_to_optimize, nb_trials=20, nb_workers=2)  Parallelize the trials across  nb_workers  cpus. Argument passed into\nthe  function_to_optimize  is the  trial_params  argument.  Example  # parallelize tuning on 2 cpus\n# this will place each trial in n into a given gpu\ndef train_main(trial_params):\n    # train your model, etc here...\n\nhparams = parser.parse_args()\nhparams.optimize_parallel_cpu(train_main, nb_trials=20, nb_workers=2)\n\n# at the end of the optimize_parallel function, all 20 trials will be completed\n# in this case by running 10 sets of 2 trials in parallel",
            "title": "optimize_parallel_cpu"
        }
    ]
}